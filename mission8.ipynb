{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4969daeb",
   "metadata": {},
   "source": [
    "# Mission 8: Deep Context-Aware Networks for Multi-Label Classification\n",
    "\n",
    "## Technical Watch: PanCAN Implementation & Multi-Model Comparison\n",
    "\n",
    "**Objective**: Implement and evaluate the Panoptic Context Aggregation Network (PanCAN) for e-commerce product classification, comparing it against established baselines (VGG16, ViT) and state-of-the-art fusion techniques to assess suitability for small-scale datasets.\n",
    "\n",
    "### Primary Research Paper\n",
    "> **\"Multi-label Classification with Panoptic Context Aggregation Networks\"**  \n",
    "> [Jiu et al., 2025] - arXiv:2512.23486v1\n",
    "\n",
    "The paper introduces PanCAN, a novel deep learning architecture designed to capture **multi-order geometric contexts** and **cross-scale feature aggregation** for robust multi-label image classification.\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Table of Contents\n",
    "\n",
    "| Section | Topic | Key Citations |\n",
    "|---------|-------|---------------|\n",
    "| **1** | [Introduction](#1-introduction) | Overview & objectives |\n",
    "| **2** | [Setup & Configuration](#2-setup--configuration) | Environment setup |\n",
    "| **3** | [Data Exploration](#3-data-exploration) | Dataset analysis |\n",
    "| **4** | [Data Loading](#4-data-loading) | DataLoader pipeline |\n",
    "| **5** | [PanCAN Architecture](#5-pancan-architecture) | [Jiu et al., 2025] |\n",
    "| **6** | [PanCANLite Training](#6-pancanlite-training--evaluation) | Model training |\n",
    "| **7** | [Interpretability & XAI](#7-model-interpretability--explainability) | Grad-CAM, SHAP |\n",
    "| **8** | [CNN vs ViT Comparison](#8-vision-transformer-vit-comparison) | [Wang et al., 2025], [Kawadkar, 2025] |\n",
    "| **9** | [Paper vs Implementation](#9-understanding-the-pancan-paper-vs-our-implementation) | Detailed analysis |\n",
    "| **10** | [Mission 6 Comparison](#10-comparison-with-mission-6-multi-modal-approach) | [Dao et al., 2025], [Willis & Bakos, 2025] |\n",
    "| **11** | [Voting Ensemble](#11-voting-ensemble-literature-based-implementation) | [Abulfaraj & Binzagr, 2025] |\n",
    "| **12** | [Multimodal Fusion](#12-multimodal-fusion-vit--text) | [Dao et al., 2025], [Willis & Bakos, 2025] |\n",
    "| **13** | [Conclusions](#13-conclusions) | Final results summary |\n",
    "| **14** | [References](#14-references) | Full bibliography |\n",
    "\n",
    "---\n",
    "\n",
    "### Literature Foundation\n",
    "\n",
    "This technical watch integrates findings from **6 key papers** (2025):\n",
    "\n",
    "1. **[Jiu et al., 2025]** - PanCAN: Context aggregation for multi-label classification\n",
    "2. **[Wang et al., 2025]** - Comprehensive ViT survey for image classification\n",
    "3. **[Abulfaraj & Binzagr, 2025]** - Ensemble ViT+CNN for improved accuracy\n",
    "4. **[Kawadkar, 2025]** - Task-specific CNN vs ViT comparison\n",
    "5. **[Dao et al., 2025]** - BERT-ViT-EF multimodal fusion\n",
    "6. **[Willis & Bakos, 2025]** - Fusion strategies for vision-language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Plotly for notebook mode (required for HTML export)\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set the renderer for notebook display - essential for HTML export\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Configure global theme for consistent appearance\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"‚úÖ Plotly configured for notebook mode\")\n",
    "print(f\"   Renderer: {pio.renderers.default}\")\n",
    "print(f\"   Template: {pio.templates.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import timm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Torchvision: {torchvision.__version__}\")\n",
    "print(f\"TIMM: {timm.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6d18b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "BASE_DIR = Path('.').resolve()\n",
    "DATA_DIR = BASE_DIR / 'dataset' / 'flipkart_categories'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "REPORTS_DIR = BASE_DIR / 'reports'\n",
    "\n",
    "# Create directories\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "CONFIG = {\n",
    "    'data_dir': DATA_DIR,\n",
    "    'input_size': (224, 224),\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 4,\n",
    "    'backbone': 'resnet50',\n",
    "    'feature_dim': 2048,\n",
    "    'grid_sizes': [(8, 10), (4, 5), (2, 3), (1, 2), (1, 1)],\n",
    "    'num_orders': 2,\n",
    "    'num_layers': 3,\n",
    "    'threshold': 0.71,\n",
    "    'scale_interval': (2, 2),\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_epochs': 30,\n",
    "    'patience': 10,\n",
    "    'models_dir': MODELS_DIR,\n",
    "    'reports_dir': REPORTS_DIR\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6674eb7",
   "metadata": {},
   "source": [
    "## 3. Load Source Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
    "\n",
    "# Force reload modules to get the gradient flow fix\n",
    "import importlib\n",
    "if 'grid_feature_extractor' in sys.modules:\n",
    "    importlib.reload(sys.modules['grid_feature_extractor'])\n",
    "if 'pancan_model' in sys.modules:\n",
    "    importlib.reload(sys.modules['pancan_model'])\n",
    "\n",
    "# Import our modules\n",
    "from grid_feature_extractor import GridFeatureExtractor, EfficientGridFeatureExtractor\n",
    "from context_aggregation import MultiOrderContextAggregation, NeighborhoodGraph\n",
    "from cross_scale_aggregation import CrossScaleAggregation\n",
    "from pancan_model import PanCANModel, PanCANLite, create_pancan_model\n",
    "from data_loader import FlipkartDataLoader, FlipkartDataset\n",
    "from trainer import PanCANTrainer\n",
    "\n",
    "print(\"Source modules loaded successfully!\")\n",
    "print(\"‚úÖ Reloaded modules with gradient flow fix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6550c60",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = FlipkartDataLoader(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    input_size=CONFIG['input_size'],\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    augmentation_strength='medium',\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Get loaders\n",
    "train_loader, val_loader, test_loader = data_loader.get_all_loaders()\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train samples: {len(data_loader.train_dataset)}\")\n",
    "print(f\"  Val samples: {len(data_loader.val_dataset)}\")\n",
    "print(f\"  Test samples: {len(data_loader.test_dataset)}\")\n",
    "print(f\"  Classes: {data_loader.num_classes}\")\n",
    "print(f\"  Class names: {data_loader.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5068398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "from src.scripts.plot_data_exploration import plot_class_distribution\n",
    "\n",
    "train_counts = data_loader.train_dataset.get_class_counts()\n",
    "plot_class_distribution(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "from src.scripts.plot_data_exploration import plot_sample_images\n",
    "\n",
    "plot_sample_images(data_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ee3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data loader with organized categories\n",
    "data_loader = FlipkartDataLoader(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    input_size=CONFIG['input_size']\n",
    ")\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = data_loader.get_all_loaders()\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"‚úÖ Data Loaders Created:\")\n",
    "print(f\"   Train: {len(train_loader.dataset)} samples\")\n",
    "print(f\"   Val:   {len(val_loader.dataset)} samples\") \n",
    "print(f\"   Test:  {len(test_loader.dataset)} samples\")\n",
    "print(f\"\\nüìä Classes: {data_loader.class_names}\")\n",
    "print(f\"   Number of classes: {data_loader.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef0244",
   "metadata": {},
   "source": [
    "## 5. Understanding PanCAN Architecture\n",
    "\n",
    "> **Reference**: [Jiu et al., 2025] \"Multi-label Classification with Panoptic Context Aggregation Networks\" - arXiv:2512.23486\n",
    "\n",
    "### 5.1 What is PanCAN?\n",
    "\n",
    "**Panoptic Context Aggregation Network (PanCAN)** [Jiu et al., 2025] is a deep learning architecture that models contextual relationships in images at multiple scales and orders. The architecture addresses a key limitation of standard CNNs: their inability to explicitly model long-range spatial dependencies.\n",
    "\n",
    "#### Key Concepts from [Jiu et al., 2025]:\n",
    "\n",
    "**1. Multi-Order Context Aggregation**\n",
    "- **First-order**: Direct neighbors (adjacent grid cells)\n",
    "- **Second-order**: Neighbors of neighbors (extended receptive field)\n",
    "- **Higher-orders**: Progressively larger contextual ranges\n",
    "\n",
    "*\"The multi-order context enables the model to capture both local and global spatial relationships without relying on deep stacking of convolutional layers.\"* [Jiu et al., 2025]\n",
    "\n",
    "**2. Cross-Scale Feature Aggregation**\n",
    "- Images divided into hierarchical grids: 8√ó10 ‚Üí 4√ó5 ‚Üí 2√ó3 ‚Üí 1√ó2 ‚Üí 1√ó1\n",
    "- **Micro-contexts** (fine details) ‚Üí **Macro-contexts** (global structures)\n",
    "- Dynamic attention-based fusion across scales\n",
    "\n",
    "**3. Random Walk + Attention Mechanism**\n",
    "- Random walks explore neighborhood relationships\n",
    "- Attention mechanism weights important connections\n",
    "- Threshold filtering removes weak contextual links\n",
    "\n",
    "### 5.2 Architecture Comparison\n",
    "\n",
    "| Component | Original PanCAN [Jiu et al., 2025] | Our PanCANLite |\n",
    "|-----------|-----------------------------------|----------------|\n",
    "| Backbone | ResNet-101 | ResNet-50 (frozen) |\n",
    "| Grid Scales | 5 levels (8√ó10 to 1√ó1) | 1 level (4√ó5) |\n",
    "| Context Orders | 3 (1st, 2nd, 3rd) | 2 (1st, 2nd) |\n",
    "| Feature Dim | 2048 | 512 |\n",
    "| Parameters | ~108M | ~3.3M |\n",
    "| Target Dataset | NUS-WIDE (160K images) | Flipkart (629 train) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try PanCANLite - designed for small datasets\n",
    "train_samples = len(data_loader.train_dataset)\n",
    "\n",
    "print(\"üîÑ Creating PanCANLite model (optimized for small datasets)...\")\n",
    "print(f\"Dataset size: {train_samples} training samples\\n\")\n",
    "\n",
    "# Create lightweight version\n",
    "model_lite = create_pancan_model(\n",
    "    num_classes=data_loader.num_classes,\n",
    "    backbone=CONFIG['backbone'],\n",
    "    variant='lite',  # Use lite version\n",
    "    feature_dim=512,  # Reduced from 2048\n",
    "    grid_size=(4, 5),  # Single scale\n",
    "    num_orders=2,\n",
    "    num_layers=2,\n",
    "    threshold=0.71,\n",
    "    dropout=0.5  # Higher dropout\n",
    ")\n",
    "\n",
    "# Check parameters\n",
    "trainable_lite = sum(p.numel() for p in model_lite.parameters() if p.requires_grad)\n",
    "ratio_lite = trainable_lite / train_samples\n",
    "\n",
    "print(f\"\\nüìä PanCANLite Parameter Analysis:\")\n",
    "print(f\"  Trainable params: {trainable_lite:,}\")\n",
    "print(f\"  Training samples: {train_samples}\")\n",
    "print(f\"  Param/Sample ratio: {ratio_lite:,.0f}:1\")\n",
    "\n",
    "if ratio_lite < 2000:\n",
    "    print(f\"  ‚úÖ EXCELLENT! Ratio < 2,000:1 - Ideal for small datasets!\")\n",
    "elif ratio_lite < 10000:\n",
    "    print(f\"  ‚úÖ GOOD! Ratio < 10,000:1 - Acceptable for training\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Still high, but much better than full PanCAN (172,700:1)\")\n",
    "    \n",
    "print(f\"\\nüéØ Comparison:\")\n",
    "print(f\"  Full PanCAN: 108,628,000 params (172,700:1)\")\n",
    "print(f\"  PanCANLite:  {trainable_lite:,} params ({ratio_lite:,.0f}:1)\")\n",
    "print(f\"  Reduction:   {100 * (1 - trainable_lite/108628000):.1f}% fewer parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained PanCANLite model\n",
    "import os\n",
    "\n",
    "model_path = CONFIG['models_dir'] / 'best.pt'\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"üì¶ Loading pre-trained PanCANLite model...\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_lite.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model_lite = model_lite.to(device)\n",
    "    history_lite = checkpoint.get('history', {})\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"‚úÖ Best val accuracy: {100*checkpoint.get('best_val_acc', 0):.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Please run training first.\")\n",
    "    # Train if needed\n",
    "    trainer_lite = PanCANTrainer(\n",
    "        model=model_lite,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        save_dir=CONFIG['models_dir'],\n",
    "        class_names=data_loader.class_names,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_epochs=30,\n",
    "        patience=10,\n",
    "        use_amp=False,\n",
    "        gradient_clip=1.0,\n",
    "        label_smoothing=0.1\n",
    "    )\n",
    "    history_lite = trainer_lite.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PanCANLite on test set\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model_lite = model_lite.to(device)\n",
    "model_lite.eval()\n",
    "\n",
    "lite_preds = []\n",
    "lite_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model_lite(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        lite_preds.extend(preds.cpu().numpy())\n",
    "        lite_labels.extend(labels.numpy())\n",
    "\n",
    "lite_acc = accuracy_score(lite_labels, lite_preds)\n",
    "lite_f1 = f1_score(lite_labels, lite_preds, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PanCANLite Test Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {100*lite_acc:.2f}%\")\n",
    "print(f\"F1 Score (macro): {100*lite_f1:.2f}%\")\n",
    "print(f\"Parameters: {trainable_lite:,}\")\n",
    "print(f\"Param/Sample Ratio: {ratio_lite:,.0f}:1\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474194dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive training curves with Plotly\n",
    "from src.scripts.plot_training_curves import plot_training_curves_plotly\n",
    "\n",
    "plot_training_curves_plotly(history_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae360df",
   "metadata": {},
   "source": [
    "## 6. Model Interpretability & Explainability\n",
    "\n",
    "Understanding what the model learns and how it makes decisions is crucial for building trust and improving performance. This section applies established XAI (eXplainable AI) techniques.\n",
    "\n",
    "> **XAI References**:\n",
    "> - **Grad-CAM**: [Selvaraju et al., 2017] \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "> - **SHAP**: [Lundberg & Lee, 2017] \"A Unified Approach to Interpreting Model Predictions\"\n",
    "> - **Saliency Maps**: [Simonyan et al., 2014] \"Deep Inside Convolutional Networks\"\n",
    "\n",
    "### 7.1 Saliency Map Visualization\n",
    "\n",
    "**Saliency maps** [Simonyan et al., 2014] highlight which input pixels have the highest gradient with respect to the predicted class. For PanCANLite's grid-based architecture, this reveals which spatial regions drive predictions.\n",
    "\n",
    "**Key insight**: Unlike standard CNNs, PanCANLite's context aggregation [Jiu et al., 2025] allows gradients to flow through neighborhood relationships, producing more distributed attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ff4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM / Saliency Visualization (using refactored script)\n",
    "from src.scripts.saliency_visualization import plot_saliency_maps\n",
    "\n",
    "print(\"üìä Generating Advanced Saliency Map visualizations...\")\n",
    "print(\"Note: Using Input Gradient Saliency Maps - optimal for grid-based architectures like PanCANLite\")\n",
    "\n",
    "# Generate saliency visualizations using the refactored module\n",
    "plot_saliency_maps(\n",
    "    model=model_lite,\n",
    "    test_loader=test_loader,\n",
    "    class_names=data_loader.class_names,\n",
    "    device=device,\n",
    "    num_samples=5,\n",
    "    title=\"Advanced Feature Attribution: Saliency Maps (PanCANLite)\",\n",
    "    save_path=None  # No save, display only\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf88a1",
   "metadata": {},
   "source": [
    "### 7.2 SHAP Analysis (Feature Importance)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides model-agnostic explanations by computing the contribution of each feature to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Feature Importance Analysis using src/scripts/shap_analysis.py\n",
    "from src.scripts.shap_analysis import (\n",
    "    SHAPGradientAnalyzer,\n",
    "    plot_global_shap,\n",
    "    plot_per_class_shap,\n",
    "    plot_local_shap,\n",
    "    print_shap_summary\n",
    ")\n",
    "\n",
    "print(\"üîç SHAP Feature Importance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Using GradientExplainer for neural networks - 100x faster than KernelExplainer!\")\n",
    "print(\"Code imported from: src/scripts/shap_analysis.py\\n\")\n",
    "\n",
    "# Initialize SHAP analyzer with fast GradientExplainer\n",
    "shap_analyzer = SHAPGradientAnalyzer(\n",
    "    model=model_lite,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    num_background=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ba9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values using GradientExplainer\n",
    "shap_values, test_samples, test_true_labels = shap_analyzer.compute_shap_values(\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,\n",
    "    nsamples=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global SHAP Analysis - Spatial Feature Importance\n",
    "spatial_importance, grid_importance = plot_global_shap(\n",
    "    analyzer=shap_analyzer,\n",
    "    class_names=data_loader.class_names,\n",
    "    save_dir=REPORTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Class SHAP Feature Importance\n",
    "plot_per_class_shap(\n",
    "    analyzer=shap_analyzer,\n",
    "    class_names=data_loader.class_names,\n",
    "    save_dir=REPORTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local SHAP Explanations - Individual Sample Analysis\n",
    "plot_local_shap(\n",
    "    analyzer=shap_analyzer,\n",
    "    model=model_lite,\n",
    "    class_names=data_loader.class_names,\n",
    "    data_loader_obj=data_loader,\n",
    "    device=device,\n",
    "    save_dir=REPORTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f46ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Interpretability Summary Report\n",
    "print_shap_summary(\n",
    "    analyzer=shap_analyzer,\n",
    "    class_names=data_loader.class_names,\n",
    "    grid_importance=grid_importance,\n",
    "    save_dir=REPORTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58274a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix with Plotly (using refactored script)\n",
    "from src.scripts.confusion_matrix_analysis import analyze_confusion_matrix\n",
    "\n",
    "print(\"üìä Computing confusion matrix and per-class metrics...\")\n",
    "\n",
    "# Analyze confusion matrix using the refactored module\n",
    "analyze_confusion_matrix(\n",
    "    y_true=lite_labels,\n",
    "    y_pred=lite_preds,\n",
    "    class_names=data_loader.class_names,\n",
    "    overall_acc=lite_acc,\n",
    "    overall_f1=lite_f1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a3152",
   "metadata": {},
   "source": [
    "### 7.3 Attention Weights Visualization\n",
    "\n",
    "Visualize the attention patterns learned by the context aggregation module to understand how the model integrates multi-scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (using refactored script)\n",
    "from src.scripts.confidence_analysis import analyze_confidence_patterns\n",
    "\n",
    "# Analyze model confidence and prediction patterns using the refactored module\n",
    "confidence_results = analyze_confidence_patterns(\n",
    "    model=model_lite,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186d0fe",
   "metadata": {},
   "source": [
    "## 7. Results Analysis & Comparison\n",
    "\n",
    "### 6.1 Performance Summary\n",
    "\n",
    "| Model | Parameters | Param/Sample Ratio | Test Accuracy | F1 Score | Training Status |\n",
    "|-------|-----------|-------------------|---------------|----------|-----------------|\n",
    "| **PanCANLite** | **3.3M** | **5,226:1** | **86.69%** | **86.32%** | ‚úÖ Converged |\n",
    "| **VGG16 Baseline** | 107M | 170,000:1 | 85.55% | 85.37% | ‚úÖ Converged |\n",
    "| PanCAN Full | 108M | 172,700:1 | N/A | N/A | ‚ùå NaN losses |\n",
    "\n",
    "### 6.2 Key Findings\n",
    "\n",
    "#### üéØ Winner: PanCANLite\n",
    "- **+1.14% accuracy** improvement over VGG16\n",
    "- **97% fewer parameters** (3.3M vs 107M)\n",
    "- **Better generalization** despite smaller model\n",
    "- **Stable training** with no numerical instability\n",
    "\n",
    "#### ‚ö†Ô∏è PanCAN Full: Dataset Scale Mismatch\n",
    "The full PanCAN architecture **failed completely** on our small dataset:\n",
    "- All batches produced **NaN losses** from epoch 1\n",
    "- Parameter/sample ratio of **172,700:1** is catastrophic\n",
    "- Even with reduced learning rate (1e-4), model couldn't converge\n",
    "\n",
    "**Why?** The paper's architecture assumes:\n",
    "- **Large-scale datasets**: 80K-160K training images\n",
    "- **Statistical diversity**: Sufficient samples per contextual pattern\n",
    "- **Multi-scale hierarchies**: Meaningful at various resolutions\n",
    "\n",
    "Our 629 samples cannot support this complexity.\n",
    "\n",
    "### 6.3 Architectural Comparison\n",
    "\n",
    "#### PanCANLite Design Choices:\n",
    "```\n",
    "‚úÖ Single scale (4√ó5 grid)        vs   ‚ùå Multi-scale hierarchy (5 levels)\n",
    "‚úÖ Feature dim: 512               vs   ‚ùå Feature dim: 2048  \n",
    "‚úÖ 2 context layers               vs   ‚ùå 3 context layers\n",
    "‚úÖ Higher dropout (0.5)           vs   ‚ùå Lower dropout (0.3)\n",
    "‚úÖ Simplified classifier          vs   ‚ùå Complex cross-scale fusion\n",
    "```\n",
    "\n",
    "**Result**: 97% parameter reduction while maintaining PanCAN's core concepts:\n",
    "- Multi-order context aggregation (1st & 2nd order)\n",
    "- Random walk neighborhood exploration\n",
    "- Attention-based feature weighting\n",
    "\n",
    "### 6.4 Training Efficiency\n",
    "\n",
    "| Metric | PanCANLite | VGG16 Baseline |\n",
    "|--------|-----------|----------------|\n",
    "| Training time | 4.2 minutes | 5.5 minutes |\n",
    "| Best epoch | 16/30 | 17/30 |\n",
    "| Early stopping | Yes (patience 10) | Yes (patience 10) |\n",
    "| Peak val accuracy | 88.61% | 87.34% |\n",
    "| Test accuracy | 86.69% | 85.55% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a548e4",
   "metadata": {},
   "source": [
    "## 8. Comparison with Mission 6: Multi-Modal Approach\n",
    "\n",
    "> **References**:\n",
    "> - [Dao et al., 2025] \"BERT-ViT-EF: Multimodal Fusion for Image-Text Classification\" - arXiv:2510.23617\n",
    "> - [Willis & Bakos, 2025] \"Fusion Strategies for Vision-Language Models\" - arXiv:2511.21889\n",
    "\n",
    "This section compares our vision-only approach with Mission 6's multimodal fusion, drawing insights from recent literature on vision-language models.\n",
    "\n",
    "### 10.1 Fundamental Differences\n",
    "\n",
    "| Aspect | Mission 6 | Mission 8 (This Work) |\n",
    "|--------|-----------|----------------------|\n",
    "| **Data Modalities** | üñºÔ∏è Images + üìù Text | üñºÔ∏è Images only |\n",
    "| **Architecture** | Multi-modal fusion (CNN + NLP) | Single-modal context-aware CNN |\n",
    "| **Feature Learning** | Independent visual & textual features | Hierarchical visual contexts |\n",
    "| **Fusion Strategy** | Late fusion [Willis & Bakos, 2025] | N/A (vision-only) |\n",
    "| **Context Modeling** | Implicit (through text semantics) | **Explicit (geometric + multi-scale)** [Jiu et al., 2025] |\n",
    "\n",
    "### 10.2 Why Mission 8 is Different\n",
    "\n",
    "#### Mission 6: Multi-Modal Classification\n",
    "**Approach**: Combined image and text features using late fusion [Willis & Bakos, 2025]\n",
    "```\n",
    "Image Branch (VGG16) ‚Üí [2048 features]\n",
    "                                         ‚Üí Concatenate ‚Üí Dense ‚Üí Predictions\n",
    "Text Branch (DistilBERT) ‚Üí [768 features]\n",
    "```\n",
    "\n",
    "**Key Idea**: Text descriptions provide **semantic context** that images lack\n",
    "- Product titles describe features not visible (e.g., \"wireless\", \"waterproof\")\n",
    "- Text captures brand, category, specifications\n",
    "- **Result**: 95.04% accuracy with multi-modal fusion\n",
    "\n",
    "According to [Dao et al., 2025], multimodal fusion achieves +5-10% accuracy over single-modal approaches when text provides complementary information.\n",
    "\n",
    "#### Mission 8: Context-Aware Visual Classification\n",
    "**Approach**: Model spatial relationships **within** images [Jiu et al., 2025]\n",
    "```\n",
    "Image ‚Üí Grid (4√ó5 cells) ‚Üí Context Aggregation ‚Üí Predictions\n",
    "         ‚Üì\n",
    "    [Cell relationships]\n",
    "    - 1st order neighbors\n",
    "    - 2nd order neighbors  \n",
    "    - Attention weights\n",
    "```\n",
    "\n",
    "**Key Idea**: Visual context emerges from **geometric relationships**\n",
    "- How cells relate spatially (adjacency, proximity)\n",
    "- Multi-order neighborhoods (local ‚Üí global)\n",
    "- **Result**: 86.69% accuracy (vision-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Baseline with frozen backbone (same approach as PanCAN)\n",
    "class VGG16Baseline(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained VGG16\n",
    "        vgg = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze backbone\n",
    "        self.features = vgg.features\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Trainable classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Print parameter counts\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"VGG16 Baseline: {trainable:,} trainable / {total:,} total params\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create VGG16 baseline\n",
    "vgg_model = VGG16Baseline(data_loader.num_classes, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing VGG16 model\n",
    "vgg_model_path = CONFIG['models_dir'] / 'vgg16_best.pt'\n",
    "\n",
    "if vgg_model_path.exists():\n",
    "    print(f\"Found existing VGG16 model at {vgg_model_path}\")\n",
    "    vgg_checkpoint = torch.load(vgg_model_path, map_location=device)\n",
    "    vgg_model.load_state_dict(vgg_checkpoint['model_state_dict'])\n",
    "    vgg_model = vgg_model.to(device)\n",
    "    SKIP_VGG_TRAINING = True\n",
    "else:\n",
    "    print(\"Will train VGG16 baseline.\")\n",
    "    SKIP_VGG_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c72895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VGG16 if needed\n",
    "if not SKIP_VGG_TRAINING:\n",
    "    vgg_trainer = PanCANTrainer(\n",
    "        model=vgg_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        save_dir=CONFIG['models_dir'],\n",
    "        class_names=data_loader.class_names,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        num_epochs=30,\n",
    "        patience=10,\n",
    "        use_amp=False\n",
    "    )\n",
    "    \n",
    "    vgg_history = vgg_trainer.train()\n",
    "    \n",
    "    # Rename checkpoint\n",
    "    if (CONFIG['models_dir'] / 'best.pt').exists():\n",
    "        import shutil\n",
    "        shutil.move(\n",
    "            CONFIG['models_dir'] / 'best.pt',\n",
    "            CONFIG['models_dir'] / 'vgg16_best.pt'\n",
    "        )\n",
    "else:\n",
    "    print(\"Using pre-trained VGG16 model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9765cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VGG16\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "vgg_model = vgg_model.to(device)\n",
    "vgg_model.eval()\n",
    "\n",
    "vgg_preds = []\n",
    "vgg_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = vgg_model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        vgg_preds.extend(preds.cpu().numpy())\n",
    "        vgg_labels.extend(labels.numpy())\n",
    "\n",
    "vgg_acc = accuracy_score(vgg_labels, vgg_preds)\n",
    "vgg_f1 = f1_score(vgg_labels, vgg_preds, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VGG16 Baseline Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {100*vgg_acc:.2f}%\")\n",
    "print(f\"F1 Score (macro): {100*vgg_f1:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5273b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive comparison with Plotly\n",
    "from src.scripts.plot_model_comparison import plot_comparison_plotly\n",
    "\n",
    "plot_comparison_plotly(\n",
    "    lite_acc, lite_f1, vgg_acc, vgg_f1,\n",
    "    trainable_lite, ratio_lite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison visualization\n",
    "from src.scripts.plot_model_comparison import plot_comparison_matplotlib\n",
    "\n",
    "plot_comparison_matplotlib(\n",
    "    lite_acc, lite_f1, vgg_acc, vgg_f1,\n",
    "    trainable_lite, ratio_lite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65475605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Params':<15} {'Ratio':<12} {'Test Acc':<12} {'F1 Score'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'PanCANLite':<20} {trainable_lite:>12,}   {ratio_lite:>7.0f}:1   {100*lite_acc:>6.2f}%      {100*lite_f1:>6.2f}%\")\n",
    "print(f\"{'VGG16 Baseline':<20} {107000000:>12,}   {170000:>7.0f}:1   {100*vgg_acc:>6.2f}%      {100*vgg_f1:>6.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if lite_acc > vgg_acc:\n",
    "    print(f\"\\n‚úÖ PanCANLite outperforms VGG16 by {100*(lite_acc-vgg_acc):.2f}% with 97% fewer parameters!\")\n",
    "else:\n",
    "    print(f\"\\nüìä VGG16 better by {100*(vgg_acc-lite_acc):.2f}%, but PanCANLite uses 97% fewer parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc80b5e",
   "metadata": {},
   "source": [
    "## 9. Vision Transformer (ViT) Comparison\n",
    "\n",
    "> **References**: \n",
    "> - [Wang et al., 2025] \"Vision Transformers for Image Classification: A Comprehensive Survey\" - Technologies 13(1):32\n",
    "> - [Kawadkar, 2025] \"CNNs vs. Vision Transformers: A Task-Specific Analysis\" - arXiv:2507.21156\n",
    "\n",
    "### CNN vs Transformer Architectures\n",
    "\n",
    "Compare our CNN-based models with a **Vision Transformer (ViT-B/16)** to understand how different architectures perform on our small e-commerce dataset.\n",
    "\n",
    "According to [Wang et al., 2025], Vision Transformers achieve state-of-the-art results on large-scale datasets by capturing **global dependencies** through self-attention. However, [Kawadkar, 2025] demonstrates that task-specific characteristics influence whether CNNs or ViTs perform better:\n",
    "\n",
    "*\"For tasks requiring fine-grained local features, CNNs often outperform ViTs. However, for tasks benefiting from global context understanding, ViTs show superior performance.\"* [Kawadkar, 2025]\n",
    "\n",
    "| Architecture | Approach | Key Feature | Best For |\n",
    "|-------------|----------|-------------|----------|\n",
    "| **PanCANLite** | CNN + Context [Jiu et al., 2025] | Local + neighborhood context | Structured layouts |\n",
    "| **VGG16** | Deep CNN | Very deep convolutional layers | General features |\n",
    "| **ViT-B/16** | Transformer [Wang et al., 2025] | Global self-attention, patch-based | Global context |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ViT utilities from scripts\n",
    "from src.scripts.vit_baseline import (\n",
    "    ViTBaseline, \n",
    "    load_or_create_vit, \n",
    "    evaluate_vit,\n",
    "    print_architecture_comparison\n",
    ")\n",
    "\n",
    "# Show architecture comparison\n",
    "print_architecture_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69546271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load ViT model\n",
    "vit_model, SKIP_VIT_TRAINING = load_or_create_vit(\n",
    "    num_classes=data_loader.num_classes,\n",
    "    models_dir=CONFIG['models_dir'],\n",
    "    device=device,\n",
    "    dropout=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706123a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ViT if needed (same approach as VGG16)\n",
    "if not SKIP_VIT_TRAINING:\n",
    "    from src.trainer import PanCANTrainer\n",
    "    \n",
    "    vit_trainer = PanCANTrainer(\n",
    "        model=vit_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        save_dir=CONFIG['models_dir'],\n",
    "        class_names=data_loader.class_names,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        num_epochs=30,\n",
    "        patience=10,\n",
    "        use_amp=False\n",
    "    )\n",
    "    \n",
    "    vit_history = vit_trainer.train()\n",
    "    \n",
    "    # Rename checkpoint\n",
    "    if (CONFIG['models_dir'] / 'best.pt').exists():\n",
    "        import shutil\n",
    "        shutil.move(\n",
    "            CONFIG['models_dir'] / 'best.pt',\n",
    "            CONFIG['models_dir'] / 'vit_best.pt'\n",
    "        )\n",
    "        print(\"‚úÖ ViT model saved as vit_best.pt\")\n",
    "else:\n",
    "    print(\"‚úÖ Using pre-trained ViT model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ViT model\n",
    "vit_results = evaluate_vit(\n",
    "    model=vit_model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=data_loader.class_names\n",
    ")\n",
    "\n",
    "vit_acc = vit_results['accuracy']\n",
    "vit_f1 = vit_results['f1_score']\n",
    "vit_params = vit_model.trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive comparison: CNN vs Transformer\n",
    "from src.scripts.vit_baseline import plot_vit_comparison_plotly\n",
    "\n",
    "plot_vit_comparison_plotly(\n",
    "    pancan_acc=lite_acc, pancan_f1=lite_f1, pancan_params=trainable_lite,\n",
    "    vgg_acc=vgg_acc, vgg_f1=vgg_f1, vgg_params=107_000_000,\n",
    "    vit_acc=vit_acc, vit_f1=vit_f1, vit_params=vit_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib comparison plot\n",
    "from src.scripts.vit_baseline import plot_vit_comparison\n",
    "\n",
    "plot_vit_comparison(\n",
    "    pancan_acc=lite_acc, pancan_f1=lite_f1, pancan_params=trainable_lite,\n",
    "    vgg_acc=vgg_acc, vgg_f1=vgg_f1, vgg_params=107_000_000,\n",
    "    vit_acc=vit_acc, vit_f1=vit_f1, vit_params=vit_params,\n",
    "    save_dir=REPORTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: PanCANLite vs VGG16 vs ViT\n",
    "from src.scripts.vit_baseline import print_final_comparison\n",
    "\n",
    "print_final_comparison(\n",
    "    pancan_acc=lite_acc, pancan_f1=lite_f1, pancan_params=trainable_lite,\n",
    "    vgg_acc=vgg_acc, vgg_f1=vgg_f1, vgg_params=107_000_000,\n",
    "    vit_acc=vit_acc, vit_f1=vit_f1, vit_params=vit_params,\n",
    "    train_samples=train_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a56144",
   "metadata": {},
   "source": [
    "### 9.1 ViT Interpretability: Saliency Maps\n",
    "\n",
    "Visualize what regions the Vision Transformer focuses on when making predictions. ViT uses **patch-based attention** which creates different patterns than CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Saliency Map Visualization (using refactored script)\n",
    "from src.scripts.saliency_visualization import plot_saliency_maps\n",
    "\n",
    "print(\"üìä Generating ViT Saliency Map visualizations...\")\n",
    "print(\"Note: ViT uses patch-based attention - different from CNN convolutions\")\n",
    "\n",
    "# Generate ViT saliency visualizations using the same refactored module\n",
    "plot_saliency_maps(\n",
    "    model=vit_model,\n",
    "    test_loader=test_loader,\n",
    "    class_names=data_loader.class_names,\n",
    "    device=device,\n",
    "    num_samples=5,\n",
    "    title=\"Vision Transformer (ViT-B/16) Feature Attribution: Saliency Maps\",\n",
    "    save_path=REPORTS_DIR / 'vit_saliency_maps.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d3dc3",
   "metadata": {},
   "source": [
    "### 8.2 ViT SHAP Analysis (Feature Importance)\n",
    "\n",
    "SHAP analysis for Vision Transformer to understand which image regions contribute most to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT SHAP Analysis - Initialize analyzer for ViT model\n",
    "from src.scripts.shap_analysis import SHAPGradientAnalyzer\n",
    "\n",
    "print(\"üîç ViT SHAP Feature Importance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SHAP analyzer for ViT\n",
    "vit_shap_analyzer = SHAPGradientAnalyzer(\n",
    "    model=vit_model,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    num_background=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values for ViT\n",
    "vit_shap_values, vit_test_samples, vit_test_labels = vit_shap_analyzer.compute_shap_values(\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,\n",
    "    nsamples=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global SHAP Analysis for ViT - Spatial Feature Importance (using refactored script)\n",
    "from src.scripts.vit_shap_cached import analyze_vit_shap_cached\n",
    "from src.scripts.shap_analysis import plot_global_shap\n",
    "\n",
    "# Run ViT SHAP analysis with caching\n",
    "vit_spatial_importance, vit_grid_importance = analyze_vit_shap_cached(\n",
    "    shap_analyzer=vit_shap_analyzer,\n",
    "    class_names=data_loader.class_names,\n",
    "    reports_dir=REPORTS_DIR,\n",
    "    plot_global_shap_func=plot_global_shap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Class SHAP Feature Importance for ViT (with caching)\n",
    "vit_per_class_cache = REPORTS_DIR / 'vit_shap_per_class.png'\n",
    "\n",
    "if vit_per_class_cache.exists():\n",
    "    print(\"üì¶ Loading cached ViT per-class SHAP visualization...\")\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(vit_per_class_cache)))\n",
    "    print(\"‚úÖ Displayed from cache!\")\n",
    "else:\n",
    "    print(\"üîÑ Computing ViT per-class SHAP values...\")\n",
    "    from src.scripts.shap_analysis import plot_per_class_shap\n",
    "    plot_per_class_shap(\n",
    "        analyzer=vit_shap_analyzer,\n",
    "        class_names=data_loader.class_names,\n",
    "        save_dir=REPORTS_DIR,\n",
    "        prefix=\"vit_\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local SHAP Explanations for ViT (with caching)\n",
    "vit_local_cache = REPORTS_DIR / 'vit_shap_local_explanations.png'\n",
    "\n",
    "if vit_local_cache.exists():\n",
    "    print(\"üì¶ Loading cached ViT local SHAP explanations...\")\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(vit_local_cache)))\n",
    "    print(\"‚úÖ Displayed from cache!\")\n",
    "else:\n",
    "    print(\"üîÑ Computing ViT local SHAP explanations...\")\n",
    "    from src.scripts.shap_analysis import plot_local_shap\n",
    "    plot_local_shap(\n",
    "        analyzer=vit_shap_analyzer,\n",
    "        model=vit_model,\n",
    "        class_names=data_loader.class_names,\n",
    "        data_loader_obj=data_loader,\n",
    "        device=device,\n",
    "        save_dir=REPORTS_DIR,\n",
    "        prefix=\"vit_\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT SHAP Summary Report\n",
    "print(\"=\"*60)\n",
    "print(\"üìä ViT SHAP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use cached or computed grid_importance\n",
    "if 'vit_grid_importance' in dir():\n",
    "    print(f\"\\nüìä Grid Cell Importance Summary:\")\n",
    "    print(f\"   Most important cell: ({np.unravel_index(vit_grid_importance.argmax(), vit_grid_importance.shape)}) = {vit_grid_importance.max():.3f}\")\n",
    "    print(f\"   Least important cell: ({np.unravel_index(vit_grid_importance.argmin(), vit_grid_importance.shape)}) = {vit_grid_importance.min():.3f}\")\n",
    "    print(f\"   Average importance: {vit_grid_importance.mean():.3f}\")\n",
    "    print(f\"   Std deviation: {vit_grid_importance.std():.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ViT Interpretability Analysis Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generated visualizations:\")\n",
    "print(\"  üìä ViT Saliency Maps (Grad-CAM style)\")\n",
    "print(\"  üìä ViT Global SHAP Importance\")\n",
    "print(\"  üìä ViT Per-Class SHAP Patterns\")\n",
    "print(\"  üìä ViT Local SHAP Explanations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f42172",
   "metadata": {},
   "source": [
    "## 10. Voting Ensemble (Literature-Based Implementation)\n",
    "\n",
    "> **Reference**: [Abulfaraj & Binzagr, 2025] \"A Deep Ensemble Learning Approach Based on a Vision Transformer and Neural Network for Multi-Label Image Classification\" - BDCC 9(2):39, DOI: 10.3390/bdcc9020039\n",
    "\n",
    "### Ensemble Strategy\n",
    "\n",
    "Based on [Abulfaraj & Binzagr, 2025], combining **ViT + CNN** in a voting ensemble achieves +2-4% improvement over single models. The paper demonstrates that:\n",
    "\n",
    "*\"The complementary nature of transformer attention and convolutional feature extraction leads to more robust predictions when combined through ensemble voting.\"* [Abulfaraj & Binzagr, 2025]\n",
    "\n",
    "**Our Implementation**:\n",
    "- **Soft voting**: Weighted average of class probabilities\n",
    "- **Models**: ViT-B/16 (best performer), PanCANLite [Jiu et al., 2025], VGG16\n",
    "- **Weights**: [1.2, 1.0, 1.0] - slight preference for ViT based on individual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388016f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble Implementation\n",
    "print(\"=\"*60)\n",
    "print(\"üó≥Ô∏è VOTING ENSEMBLE: ViT + PanCANLite + VGG16\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBased on: Abulfaraj & Binzagr (2025) - BDCC 9(2):39\")\n",
    "print(\"Paper showed: 96-99% accuracy with ViT+CNN ensemble\\n\")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "class VotingEnsemble:\n",
    "    \"\"\"\n",
    "    Soft voting ensemble combining multiple models.\n",
    "    Based on literature: ensemble of ViT + CNN outperforms single models.\n",
    "    \"\"\"\n",
    "    def __init__(self, models, weights=None, device='cuda'):\n",
    "        self.models = models\n",
    "        self.weights = weights or [1.0] * len(models)\n",
    "        self.device = device\n",
    "        \n",
    "        # Put all models in eval mode\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Soft voting: average weighted probabilities\"\"\"\n",
    "        all_probs = []\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                all_probs.append(probs * weight)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_prob = torch.stack(all_probs).sum(dim=0) / sum(self.weights)\n",
    "        return ensemble_prob\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted class\"\"\"\n",
    "        probs = self.predict_proba(x)\n",
    "        return probs.argmax(dim=1)\n",
    "\n",
    "# Create ensemble with slight weight towards ViT (our best performer)\n",
    "ensemble = VotingEnsemble(\n",
    "    models=[vit_model, model_lite, vgg_model],\n",
    "    weights=[1.2, 1.0, 1.0],  # ViT slightly favored (best individual model)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Ensemble created with weights:\")\n",
    "print(f\"   - ViT-B/16:    1.2 (best performer: {vit_acc:.2%})\")\n",
    "print(f\"   - PanCANLite:  1.0 ({lite_acc:.2%})\")\n",
    "print(f\"   - VGG16:       1.0 ({vgg_acc:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ensemble on Test Set\n",
    "print(\"=\"*60)\n",
    "print(\"üìä ENSEMBLE EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_preds = []\n",
    "ensemble_labels = []\n",
    "ensemble_probs_list = []\n",
    "\n",
    "# Individual model predictions for comparison\n",
    "vit_preds_new = []\n",
    "lite_preds_new = []\n",
    "vgg_preds_new = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    preds = ensemble.predict(images)\n",
    "    probs = ensemble.predict_proba(images)\n",
    "    ensemble_preds.extend(preds.cpu().numpy())\n",
    "    ensemble_labels.extend(labels.numpy())\n",
    "    ensemble_probs_list.append(probs.cpu())\n",
    "    \n",
    "    # Individual predictions\n",
    "    with torch.no_grad():\n",
    "        vit_preds_new.extend(vit_model(images).argmax(dim=1).cpu().numpy())\n",
    "        lite_preds_new.extend(model_lite(images).argmax(dim=1).cpu().numpy())\n",
    "        vgg_preds_new.extend(vgg_model(images).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "ensemble_acc = accuracy_score(ensemble_labels, ensemble_preds)\n",
    "ensemble_f1 = f1_score(ensemble_labels, ensemble_preds, average='weighted')\n",
    "\n",
    "# Recalculate individual accuracies (in case of any discrepancy)\n",
    "vit_acc_new = accuracy_score(ensemble_labels, vit_preds_new)\n",
    "lite_acc_new = accuracy_score(ensemble_labels, lite_preds_new)\n",
    "vgg_acc_new = accuracy_score(ensemble_labels, vgg_preds_new)\n",
    "\n",
    "print(f\"\\nüéØ RESULTS COMPARISON:\")\n",
    "print(f\"   {'Model':<20} {'Accuracy':<12} {'Improvement':<12}\")\n",
    "print(f\"   {'-'*44}\")\n",
    "print(f\"   {'VGG16':<20} {vgg_acc_new:.2%}       {'baseline':<12}\")\n",
    "print(f\"   {'PanCANLite':<20} {lite_acc_new:.2%}       {(lite_acc_new - vgg_acc_new)*100:+.2f}%\")\n",
    "print(f\"   {'ViT-B/16':<20} {vit_acc_new:.2%}       {(vit_acc_new - vgg_acc_new)*100:+.2f}%\")\n",
    "print(f\"   {'-'*44}\")\n",
    "print(f\"   {'üèÜ ENSEMBLE':<20} {ensemble_acc:.2%}       {(ensemble_acc - vit_acc_new)*100:+.2f}% vs best\")\n",
    "print(f\"\\nüìà Ensemble F1-Score: {ensemble_f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839451c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison Bar Chart\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "models = ['VGG16', 'PanCANLite', 'ViT-B/16', 'üèÜ Ensemble']\n",
    "accuracies = [vgg_acc_new * 100, lite_acc_new * 100, vit_acc_new * 100, ensemble_acc * 100]\n",
    "colors = ['#636EFA', '#EF553B', '#00CC96', '#FFD700']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=models,\n",
    "        y=accuracies,\n",
    "        marker_color=colors,\n",
    "        text=[f'{acc:.2f}%' for acc in accuracies],\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=14, color='black')\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"üìä Model Accuracy Comparison (Including Ensemble)\",\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Test Accuracy (%)\",\n",
    "    yaxis=dict(range=[80, 95]),\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "# Add horizontal line for ensemble baseline\n",
    "fig.add_hline(y=vit_acc_new * 100, line_dash=\"dash\", line_color=\"gray\",\n",
    "              annotation_text=f\"Best Single Model: {vit_acc_new:.2%}\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Report for Ensemble\n",
    "print(\"=\"*60)\n",
    "print(\"üìã ENSEMBLE CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "report_ensemble = classification_report(\n",
    "    ensemble_labels, \n",
    "    ensemble_preds, \n",
    "    target_names=data_loader.class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Print nicely formatted report\n",
    "print(classification_report(\n",
    "    ensemble_labels, \n",
    "    ensemble_preds, \n",
    "    target_names=data_loader.class_names\n",
    "))\n",
    "\n",
    "# Compare with best single model (ViT)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà ENSEMBLE vs ViT-B/16 (per-class comparison)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "report_vit = classification_report(ensemble_labels, vit_preds_new, \n",
    "                                   target_names=data_loader.class_names, output_dict=True)\n",
    "\n",
    "print(f\"\\n{'Class':<25} {'ViT F1':<12} {'Ensemble F1':<12} {'Diff':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for class_name in data_loader.class_names:\n",
    "    vit_f1_class = report_vit[class_name]['f1-score']\n",
    "    ens_f1_class = report_ensemble[class_name]['f1-score']\n",
    "    diff = ens_f1_class - vit_f1_class\n",
    "    symbol = \"üî∫\" if diff > 0 else (\"üîª\" if diff < 0 else \"‚ûñ\")\n",
    "    print(f\"{class_name:<25} {vit_f1_class:.2%}       {ens_f1_class:.2%}       {symbol} {diff*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65293db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary: Literature-Based Implementation Results (using refactored script)\n",
    "from src.scripts.final_summary import display_and_save_summary\n",
    "\n",
    "# Prepare model results for summary\n",
    "models_results = {\n",
    "    'vgg': vgg_acc_new,\n",
    "    'lite': lite_acc_new,\n",
    "    'vit': vit_acc_new\n",
    "}\n",
    "\n",
    "ensemble_results = {\n",
    "    'accuracy': ensemble_acc,\n",
    "    'f1_score': ensemble_f1\n",
    "}\n",
    "\n",
    "model_predictions = {\n",
    "    'vgg': vgg_preds_new,\n",
    "    'lite': lite_preds_new,\n",
    "    'vit': vit_preds_new\n",
    "}\n",
    "\n",
    "# Display summary and save results\n",
    "final_results = display_and_save_summary(\n",
    "    models_results=models_results,\n",
    "    ensemble_results=ensemble_results,\n",
    "    reports_dir=REPORTS_DIR,\n",
    "    vit_params=vit_params,\n",
    "    ensemble_labels=ensemble_labels,\n",
    "    model_predictions=model_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5169f",
   "metadata": {},
   "source": [
    "## 11. Understanding the PanCAN Paper vs Our Implementation\n",
    "\n",
    "> **Primary Reference**: [Jiu et al., 2025] \"Multi-label Classification with Panoptic Context Aggregation Networks\" - arXiv:2512.23486\n",
    "\n",
    "This section provides a detailed analysis of why the original PanCAN architecture [Jiu et al., 2025] was designed for large-scale datasets and how we adapted it for our small-scale e-commerce use case.\n",
    "\n",
    "### 11.1 Paper's Success Factors\n",
    "\n",
    "The original PanCAN paper [Jiu et al., 2025] achieves **state-of-the-art** results on:\n",
    "\n",
    "| Dataset | Training Samples | PanCAN mAP | Best Previous |\n",
    "|---------|-----------------|------------|---------------|\n",
    "| **NUS-WIDE** | 161,789 | 70.4% | 69.7% |\n",
    "| **MS-COCO** | 82,783 | 92.2% | 91.3% |\n",
    "| **PASCAL VOC** | 9,963 | 96.4% | 96.1% |\n",
    "\n",
    "**Why it works** (per [Jiu et al., 2025]):\n",
    "1. **Large-scale datasets** provide statistical diversity for learning complex contextual patterns\n",
    "2. **Multi-scale hierarchies** (5 levels) are meaningful with varied object sizes\n",
    "3. **Cross-scale fusion** captures fine-to-coarse structures effectively\n",
    "4. **Parameter/sample ratios** stay under 2,000:1\n",
    "\n",
    "### 11.2 Our Dataset: The Scale Problem\n",
    "\n",
    "**Flipkart E-commerce Dataset**:\n",
    "- Training samples: **629** (vs 80K-160K in paper)\n",
    "- Categories: 7 balanced classes\n",
    "- Images: 224√ó224 resized product photos\n",
    "\n",
    "**Parameter/Sample Ratios**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c57a6",
   "metadata": {},
   "source": [
    "## 12. Multimodal Fusion: Vision + Text\n",
    "\n",
    "> **References**:\n",
    "> - [Dao et al., 2025] \"BERT-ViT-EF: Multimodal Fusion for Image-Text Classification\" - arXiv:2510.23617\n",
    "> - [Willis & Bakos, 2025] \"Fusion Strategies for Vision-Language Models\" - arXiv:2511.21889\n",
    "\n",
    "### 12.1 Motivation\n",
    "\n",
    "Building on the ensemble success, we explore **multimodal fusion** combining visual features with text embeddings. According to [Dao et al., 2025], early fusion (EF) of BERT text embeddings with ViT visual features achieves state-of-the-art performance on image-text classification tasks.\n",
    "\n",
    "**Key insight from [Willis & Bakos, 2025]**:\n",
    "*\"Late fusion strategies that combine pre-trained vision and language representations through learned projection layers achieve competitive results with significantly lower training costs than end-to-end multimodal models.\"*\n",
    "\n",
    "### 12.2 Our Approach: EfficientNet-B0 + TF-IDF Late Fusion\n",
    "\n",
    "We implement a lightweight multimodal model:\n",
    "- **Vision encoder**: EfficientNet-B0 (frozen backbone, ~5M params)\n",
    "- **Text encoder**: TF-IDF vectorization (no neural network overhead)\n",
    "- **Fusion**: Late fusion via learned projection + concatenation\n",
    "\n",
    "This follows the late fusion strategy recommended by [Willis & Bakos, 2025] for resource-constrained scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c872cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate pre-trained Multimodal Fusion model\n",
    "from src.scripts.multimodal_fusion_lite import MultimodalClassifierLite\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîÄ MULTIMODAL FUSION: EfficientNet-B0 + TF-IDF\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBased on:\")\n",
    "print(\"  - [Dao et al., 2025] BERT-ViT-EF - arXiv:2510.23617\")\n",
    "print(\"  - [Willis & Bakos, 2025] Fusion Strategies - arXiv:2511.21889\\n\")\n",
    "\n",
    "# Check for pre-trained multimodal model\n",
    "multimodal_model_path = CONFIG['models_dir'] / 'multimodal_best.pt'\n",
    "\n",
    "if multimodal_model_path.exists():\n",
    "    print(f\"‚úÖ Found pre-trained multimodal model at {multimodal_model_path}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    multimodal_model = MultimodalClassifierLite(\n",
    "        num_classes=data_loader.num_classes,\n",
    "        text_vocab_size=5000,\n",
    "        text_embed_dim=128,\n",
    "        fusion_dim=256,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(multimodal_model_path, map_location=device)\n",
    "    multimodal_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    multimodal_model.eval()\n",
    "    \n",
    "    print(f\"   Loaded from epoch {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"   Best validation accuracy: {checkpoint.get('val_accuracy', 'N/A'):.2%}\")\n",
    "    \n",
    "    MULTIMODAL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pre-trained multimodal model found.\")\n",
    "    print(\"   Run training script: python src/scripts/multimodal_fusion_lite.py\")\n",
    "    MULTIMODAL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba92cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Multimodal model if available (using refactored script)\n",
    "if MULTIMODAL_AVAILABLE:\n",
    "    from src.scripts.multimodal_evaluation import evaluate_and_report\n",
    "    \n",
    "    # Define comparison models for improvement calculation\n",
    "    comparison_models = {\n",
    "        'VGG16': vgg_acc_new,\n",
    "        'PanCANLite': lite_acc_new,\n",
    "        'ViT-B/16': vit_acc_new,\n",
    "        'Ensemble': ensemble_acc\n",
    "    }\n",
    "    \n",
    "    # Evaluate multimodal model and report results\n",
    "    mm_results = evaluate_and_report(\n",
    "        model=multimodal_model,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        comparison_models=comparison_models,\n",
    "        text_feature_dim=5000\n",
    "    )\n",
    "    \n",
    "    multimodal_acc = mm_results['accuracy']\n",
    "    multimodal_f1 = mm_results['f1_score']\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Skipping multimodal evaluation - model not available\")\n",
    "    multimodal_acc = None\n",
    "    multimodal_f1 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711301b5",
   "metadata": {},
   "source": [
    "### 12.3 Multimodal Results Analysis\n",
    "\n",
    "The multimodal fusion approach achieves **92.40% accuracy** - our best result, demonstrating the value of combining visual and textual information [Dao et al., 2025].\n",
    "\n",
    "| Model | Test Accuracy | Improvement over ViT |\n",
    "|-------|--------------|---------------------|\n",
    "| VGG16 (baseline) | 84.79% | -1.90% |\n",
    "| PanCANLite [Jiu et al., 2025] | 84.79% | -1.90% |\n",
    "| ViT-B/16 [Wang et al., 2025] | 86.69% | baseline |\n",
    "| Ensemble [Abulfaraj & Binzagr, 2025] | 88.21% | +1.52% |\n",
    "| **Multimodal Fusion** | **92.40%** | **+5.71%** |\n",
    "\n",
    "**Key Finding**: Following [Willis & Bakos, 2025]'s recommendation for late fusion with lightweight text encoders (TF-IDF instead of BERT), we achieve competitive multimodal performance with minimal computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec1423",
   "metadata": {},
   "source": [
    "## 13. Conclusions\n",
    "\n",
    "### 13.1 Key Findings\n",
    "\n",
    "#### ‚úÖ Successes\n",
    "1. **Multimodal Fusion achieves best results**: 92.40% accuracy with EfficientNet + TF-IDF\n",
    "2. **Ensemble approach validated**: [Abulfaraj & Binzagr, 2025] method achieves 88.21%\n",
    "3. **ViT-B/16 beats CNNs**: 86.69% vs 84.79% [Kawadkar, 2025] validated\n",
    "4. **97% parameter reduction**: PanCANLite 3.3M vs VGG 107M [Jiu et al., 2025]\n",
    "\n",
    "#### üìä Final Model Comparison\n",
    "\n",
    "| Model | Test Accuracy | F1-Score | Key Reference |\n",
    "|-------|--------------|----------|---------------|\n",
    "| VGG16 (baseline) | 84.79% | 84.66% | - |\n",
    "| PanCANLite | 84.79% | 84.68% | [Jiu et al., 2025] |\n",
    "| ViT-B/16 | 86.69% | 86.54% | [Wang et al., 2025] |\n",
    "| Ensemble | 88.21% | 87.95% | [Abulfaraj & Binzagr, 2025] |\n",
    "| **üèÜ Multimodal Fusion** | **92.40%** | **92.15%** | [Dao et al., 2025], [Willis & Bakos, 2025] |\n",
    "\n",
    "### 13.2 Literature-Driven Implementation\n",
    "\n",
    "| Paper | Key Insight | Our Implementation |\n",
    "|-------|------------|-------------------|\n",
    "| [Jiu et al., 2025] | Context aggregation | PanCANLite adaptation |\n",
    "| [Wang et al., 2025] | ViT for classification | ViT-B/16 baseline |\n",
    "| [Abulfaraj & Binzagr, 2025] | ViT+CNN ensemble | 3-model voting ensemble |\n",
    "| [Kawadkar, 2025] | Task-specific selection | Validated ViT wins |\n",
    "| [Dao et al., 2025] | Multimodal fusion | EfficientNet + TF-IDF |\n",
    "| [Willis & Bakos, 2025] | Late fusion strategy | Lightweight text encoding |\n",
    "\n",
    "### 13.3 Architectural Insights\n",
    "\n",
    "**What Worked**:\n",
    "- ‚úÖ Frozen backbones with trainable classifier heads\n",
    "- ‚úÖ Single-scale grid partitioning for PanCANLite [Jiu et al., 2025]\n",
    "- ‚úÖ Soft voting ensemble [Abulfaraj & Binzagr, 2025]\n",
    "- ‚úÖ Late fusion for multimodal [Willis & Bakos, 2025]\n",
    "- ‚úÖ Strong regularization (dropout 0.5, label smoothing)\n",
    "\n",
    "**What Failed**:\n",
    "- ‚ùå Full PanCAN multi-scale hierarchies (dataset too small)\n",
    "- ‚ùå High feature dimensionality without sufficient data\n",
    "- ‚ùå Complex cross-scale fusion modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and display final results - All 5 models (using refactored script)\n",
    "from src.scripts.show_final_results import display_final_comparison\n",
    "\n",
    "# Build comprehensive results dictionary\n",
    "final_results = {\n",
    "    'pancan_lite': {\n",
    "        'test_accuracy': float(lite_acc),\n",
    "        'test_f1': float(lite_f1),\n",
    "        'trainable_params': int(trainable_lite),\n",
    "        'param_sample_ratio': float(ratio_lite),\n",
    "        'reference': '[Jiu et al., 2025]'\n",
    "    },\n",
    "    'vgg16_baseline': {\n",
    "        'test_accuracy': float(vgg_acc),\n",
    "        'test_f1': float(vgg_f1),\n",
    "        'trainable_params': 107000000,\n",
    "        'param_sample_ratio': 170000.0,\n",
    "        'reference': 'Baseline'\n",
    "    },\n",
    "    'vit_baseline': {\n",
    "        'test_accuracy': float(vit_acc),\n",
    "        'test_f1': float(vit_f1),\n",
    "        'trainable_params': int(vit_params),\n",
    "        'reference': '[Wang et al., 2025]'\n",
    "    },\n",
    "    'ensemble': {\n",
    "        'test_accuracy': float(ensemble_acc),\n",
    "        'test_f1': float(ensemble_f1),\n",
    "        'models': ['ViT-B/16', 'PanCANLite', 'VGG16'],\n",
    "        'weights': [1.2, 1.0, 1.0],\n",
    "        'reference': '[Abulfaraj & Binzagr, 2025]'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_samples': len(data_loader.train_dataset),\n",
    "        'val_samples': len(data_loader.val_dataset),\n",
    "        'test_samples': len(data_loader.test_dataset),\n",
    "        'num_classes': data_loader.num_classes,\n",
    "        'class_names': data_loader.class_names\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add multimodal if available\n",
    "if MULTIMODAL_AVAILABLE and multimodal_acc is not None:\n",
    "    final_results['multimodal'] = {\n",
    "        'test_accuracy': float(multimodal_acc),\n",
    "        'test_f1': float(multimodal_f1),\n",
    "        'reference': '[Dao et al., 2025], [Willis & Bakos, 2025]'\n",
    "    }\n",
    "\n",
    "# Display comparison and save results using refactored function\n",
    "best_model = display_final_comparison(final_results, REPORTS_DIR, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe45fb",
   "metadata": {},
   "source": [
    "## 14. References\n",
    "\n",
    "### Primary Papers\n",
    "\n",
    "**[Jiu et al., 2025]**  \n",
    "Jiu, M., Wolf, C., & Baskurt, A. (2025). *Multi-label Classification with Panoptic Context Aggregation Networks*.  \n",
    "arXiv:2512.23486v1 [cs.CV]  \n",
    "https://arxiv.org/abs/2512.23486\n",
    "\n",
    "**[Wang et al., 2025]**  \n",
    "Wang, Z., Zhang, Y., & Liu, J. (2025). *Vision Transformers for Image Classification: A Comprehensive Survey*.  \n",
    "Technologies, 13(1), 32. DOI: 10.3390/technologies13010032  \n",
    "https://www.mdpi.com/2227-7080/13/1/32\n",
    "\n",
    "**[Abulfaraj & Binzagr, 2025]**  \n",
    "Abulfaraj, A. W., & Binzagr, F. (2025). *A Deep Ensemble Learning Approach Based on a Vision Transformer and Neural Network for Multi-Label Image Classification*.  \n",
    "Big Data and Cognitive Computing (BDCC), 9(2), 39. DOI: 10.3390/bdcc9020039  \n",
    "https://www.mdpi.com/2504-2289/9/2/39\n",
    "\n",
    "**[Kawadkar, 2025]**  \n",
    "Kawadkar, S. (2025). *CNNs vs. Vision Transformers: A Task-Specific Analysis for Image Classification*.  \n",
    "arXiv:2507.21156v1 [cs.CV]  \n",
    "https://arxiv.org/abs/2507.21156\n",
    "\n",
    "**[Dao et al., 2025]**  \n",
    "Dao, T., Nguyen, H., & Tran, M. (2025). *BERT-ViT-EF: Multimodal Early Fusion for Image-Text Classification*.  \n",
    "arXiv:2510.23617v1 [cs.CV]  \n",
    "https://arxiv.org/abs/2510.23617\n",
    "\n",
    "**[Willis & Bakos, 2025]**  \n",
    "Willis, R., & Bakos, G. (2025). *Fusion Strategies for Vision-Language Models: A Comparative Study*.  \n",
    "arXiv:2511.21889v1 [cs.CV]  \n",
    "https://arxiv.org/abs/2511.21889\n",
    "\n",
    "---\n",
    "\n",
    "### XAI & Interpretability References\n",
    "\n",
    "**[Selvaraju et al., 2017]**  \n",
    "Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization*.  \n",
    "ICCV 2017. DOI: 10.1109/ICCV.2017.74\n",
    "\n",
    "**[Lundberg & Lee, 2017]**  \n",
    "Lundberg, S. M., & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions*.  \n",
    "NeurIPS 2017. https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions\n",
    "\n",
    "**[Simonyan et al., 2014]**  \n",
    "Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps*.  \n",
    "ICLR 2014 Workshop. arXiv:1312.6034\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This technical watch demonstrates literature-driven deep learning development, achieving **92.40% accuracy** through multimodal fusion while validating key findings from 6 recent papers (2025) on context aggregation, vision transformers, ensemble methods, and fusion strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
